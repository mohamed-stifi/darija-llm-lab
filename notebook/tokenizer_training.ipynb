{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd990f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mohamed-stifi/darija-combined-dataset\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc3efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36431a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'google/gemma-3n-E2B-it'\n",
    "current_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id, trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae76a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_tokens = set(current_tokenizer.vocab.keys())\n",
    "print(len(existing_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac8087",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataset[30000]['text']\n",
    "\n",
    "print(current_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b35730",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(current_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dataset['text']\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01fa980",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tokens = []\n",
    "text_lenght = []\n",
    "for text in texts:\n",
    "    text_lenght.append(len(text))\n",
    "    number_of_tokens.append(len(current_tokenizer.tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7710a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure the lengths are calculated before plotting\n",
    "# Assuming 'text_lenght' and 'number_of_tokens' lists exist from the preceding code\n",
    "\n",
    "# Plotting the relationship between text length and number of tokens\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=text_lenght, y=number_of_tokens)\n",
    "plt.title('Number of Tokens vs Text Length')\n",
    "plt.xlabel('Text Length (Characters)')\n",
    "plt.ylabel('Number of Tokens')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Additional plots (histograms for distribution of text length and token count)\n",
    "\n",
    "# Histogram of Text Length\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(text_lenght, bins=50, kde=True)\n",
    "plt.title('Distribution of Text Length')\n",
    "plt.xlabel('Text Length (Characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Histogram of Number of Tokens\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(number_of_tokens, bins=50, kde=True)\n",
    "plt.title('Distribution of Number of Tokens')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa0f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for text in texts:\n",
    "    words.extend(text.split())\n",
    "\n",
    "words = set(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = len(words)//4\n",
    "print(f\"vocabolary size: {voc_size}\")\n",
    "new_tokenizer = current_tokenizer.train_new_from_iterator(texts,voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataset[30000]['text']\n",
    "\n",
    "print(new_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_tokenizer.tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8e5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tokens = []\n",
    "text_lenght = []\n",
    "for text in texts:\n",
    "    text_lenght.append(len(text))\n",
    "    number_of_tokens.append(len(new_tokenizer.tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure the lengths are calculated before plotting\n",
    "# Assuming 'text_lenght' and 'number_of_tokens' lists exist from the preceding code\n",
    "\n",
    "# Plotting the relationship between text length and number of tokens\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=text_lenght, y=number_of_tokens)\n",
    "plt.title('Number of Tokens vs Text Length')\n",
    "plt.xlabel('Text Length (Characters)')\n",
    "plt.ylabel('Number of Tokens')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Additional plots (histograms for distribution of text length and token count)\n",
    "\n",
    "# Histogram of Text Length\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(text_lenght, bins=50, kde=True)\n",
    "plt.title('Distribution of Text Length')\n",
    "plt.xlabel('Text Length (Characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Histogram of Number of Tokens\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(number_of_tokens, bins=50, kde=True)\n",
    "plt.title('Distribution of Number of Tokens')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_tokens_len = []\n",
    "for token in current_tokenizer.vocab.keys():\n",
    "    existing_tokens_len.append(len(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: plot the ferquences of lenghts in existing_tokens_len\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plotting the frequencies of lengths of existing tokens\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(existing_tokens_len, bins=50, kde=True)\n",
    "plt.title('Distribution of Lengths of Existing Tokens')\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1394ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_token_len = []\n",
    "for token in new_tokenizer.vocab.keys():\n",
    "    new_token_len.append(len(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4905c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting the frequencies of lengths of existing tokens\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(new_token_len, bins=50, kde=True)\n",
    "plt.title('Distribution of Lengths of New Tokens')\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a689d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove the tokens with len > 30\n",
    "\n",
    "new_tokens_to_remove = {}\n",
    "new_tokens_to_keep = {}\n",
    "\n",
    "for token, id in new_tokenizer.vocab.items():\n",
    "    if len(token) > 30:\n",
    "        new_tokens_to_remove[token] = id\n",
    "    else:\n",
    "        new_tokens_to_keep[token] = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c818060",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_tokens_to_remove), len(new_tokens_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e815db",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_token_to_keep_len = []\n",
    "for token in new_tokens_to_keep.keys():\n",
    "    new_token_to_keep_len.append(len(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8816a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting the frequencies of lengths of existing tokens\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(new_token_to_keep_len, bins=50, kde=True)\n",
    "plt.title('Distribution of Lengths of New Tokens To Keep')\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc3bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the vocab keys to sets for efficient lookup\n",
    "existing_tokens_set = set(current_tokenizer.vocab.keys())\n",
    "new_tokens_to_keep_set = set(new_tokens_to_keep.keys())\n",
    "\n",
    "# Find tokens in new_tokens_to_keep that are also in current_tokenizer.vocab\n",
    "tokens_in_both = new_tokens_to_keep_set.intersection(existing_tokens_set)\n",
    "\n",
    "# Find tokens in new_tokens_to_keep that are NOT in current_tokenizer.vocab\n",
    "tokens_only_in_new = new_tokens_to_keep_set.difference(existing_tokens_set)\n",
    "\n",
    "print(f\"Tokens in new_tokens_to_keep that are in current_tokenizer.vocab: {len(tokens_in_both)}\")\n",
    "# print(tokens_in_both) # Uncomment to print the tokens themselves\n",
    "\n",
    "print(f\"Tokens in new_tokens_to_keep that are NOT in current_tokenizer.vocab: {len(tokens_only_in_new)}\")\n",
    "# print(tokens_only_in_new) # Uncomment to print the tokens themselves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d3aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_in_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3593c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_only_in_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_tokenizer.add_tokens(list(tokens_only_in_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_tokens = set(current_tokenizer.vocab.keys())\n",
    "print(len(existing_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd28696",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataset[30000]['text']\n",
    "\n",
    "print(new_tokenizer.tokenize(text))\n",
    "len(new_tokenizer.tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c021bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_save_path = \"./darija_gemma_3n_tokenizer\"\n",
    "current_tokenizer.save_pretrained(tokenizer_save_path)\n",
    "print(f\"Tokenizer saved locally at: {tokenizer_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a30ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Set your repository ID (format: \"username/repo_name\")\n",
    "tokenizer_repo_id = \"mohamed-stifi/darija_gemma_3n_tokenizer\"  # Customize the repo name\n",
    "\n",
    "# Push to Hub\n",
    "current_tokenizer.push_to_hub(\n",
    "    repo_id=tokenizer_repo_id,\n",
    "    commit_message=\"Add Darija-adapted Gemma tokenizer from dataset mohamed-stifi/darija-combined-dataset\",\n",
    ")\n",
    "print(f\"✅ Tokenizer uploaded to: https://huggingface.co/{tokenizer_repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ecfdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".darija-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
